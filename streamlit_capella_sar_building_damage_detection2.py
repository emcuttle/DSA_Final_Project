# -*- coding: utf-8 -*-
"""streamlit_capella_sar_building_damage_detection2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qISvaUpVupUcovSuePGYel6bDlp7GoXG

***Synthetic Aperture Radar (SAR) Building Damage Classifier***

*   List item
*   List item



-
"""

# Pip install modules:
# !pip install fiona rasterio folium matplotlib mapclassify optuna ultralytics -q

# Imports:
import os
import shutil
import rasterio
from rasterio.mask import mask
import numpy as np
from PIL import Image # Using Pillow to save images
from sklearn.model_selection import train_test_split
from tqdm import tqdm
import geopandas as gpd
import json
import pandas as pd
import requests
from shapely import wkt
import matplotlib.pyplot as plt
import zipfile

from shapely.geometry import box
from rasterio.errors import RasterioIOError

# Mount Google Drive:
# from google.colab import drive
# drive.mount('/content/drive/')

# Define functions:

def get_download_url(url):
  r = requests.get(url)
  data = r.json()
  return data['resultUrl']

def download_file(url, local_filename):
  r = requests.get(url, stream=True)
  with open(local_filename, 'wb') as f:
    for chunk in r.iter_content(chunk_size=8192):
      if chunk:  # Filter out keep-alive new chunks
        f.write(chunk)

def _process_and_save(df, split_name, raster_src, output_dir, target_size):
    """
    Internal helper function to process a GeoDataFrame split, extract/pad images,
    and save them to the correct directory.

    Args:
        df (gpd.GeoDataFrame): The GeoDataFrame for the current split (train/val/test).
        split_name (str): The name of the split ('train', 'val', or 'test').
        raster_src (rasterio.DatasetReader): The opened rasterio source object.
        output_dir (str): The root directory for the prepared dataset.
        target_size (tuple): The (height, width) for the final padded images.
    """
    for index, row in tqdm(df.iterrows(), total=df.shape[0], desc=f"Processing {split_name} set"):
        geom = [row.geometry]
        label = row['label']
        image_name = f"building_{index}.png"

        try:
            # Extract the image chip using the building's geometry
            out_image, out_transform = mask(raster_src, geom, crop=True, nodata=0)
            out_image = out_image[0] # Select the first band

            # Skip if the geometry didn't overlap with the raster
            if out_image.size == 0:
                continue

            # --- Normalize the image to 8-bit (0-255) ---
            valid_pixels = out_image[out_image != 0]
            if valid_pixels.size > 0:
                # Use 2nd and 98th percentiles to handle outliers
                vmin, vmax = np.percentile(valid_pixels, (2, 98))
                if vmax > vmin:
                    out_image = np.clip(out_image, vmin, vmax)
                    out_image = ((out_image - vmin) / (vmax - vmin) * 255).astype(np.uint8)
                else: # Handle cases with no contrast
                    out_image = np.full(out_image.shape, 128, dtype=np.uint8)
            else: # Handle cases with only nodata pixels in the crop
                 out_image = np.zeros(out_image.shape, dtype=np.uint8)


            # --- Pad the image to the target size ---
            h, w = out_image.shape
            canvas = np.zeros(target_size, dtype=np.uint8)
            # Center the chip on the canvas
            y_offset = (target_size[0] - h) // 2
            x_offset = (target_size[1] - w) // 2
            canvas[y_offset:y_offset+h, x_offset:x_offset+w] = out_image

            # --- Save the final image ---
            output_path = os.path.join(output_dir, split_name, str(label), image_name)
            Image.fromarray(canvas).save(output_path)

        except Exception as e:
            print(f"Error processing building {index}: {e}")
            continue

def _safe_stratified_split(gdf, test_size, random_state):
    """
    Wrapper for train_test_split that handles stratification errors
    if a class has too few members.
    """
    # Handle edge cases for 0% or 100% splits
    if gdf.empty:
        return gdf.copy(), gdf.copy() # Return two empty GDFs
    if test_size == 0.0:
        return gdf.copy(), gdf.head(0).copy() # Return all data and an empty GDF
    if test_size == 1.0:
        return gdf.head(0).copy(), gdf.copy() # Return empty GDF and all data

    try:
        # Try to split with stratification
        train_gdf, test_gdf = train_test_split(
            gdf,
            test_size=test_size,
            random_state=random_state,
            stratify=gdf['label']
        )
    except ValueError as e:
        # If stratification fails (e.g., only 1 sample in a class)
        if "The least populated class" in str(e):
            print(f"Warning: Stratification failed ({e}). Proceeding with non-stratified split.")
            train_gdf, test_gdf = train_test_split(
                gdf,
                test_size=test_size,
                random_state=random_state,
                stratify=None # Fallback to no stratification
            )
        else:
            # Re-raise other unexpected errors
            raise e
    return train_gdf, test_gdf

def prepare_sar_dataset(gdf, raster_file_path, output_dir, target_size=(256, 256),
                          split_ratios=(0.7, 0.15, 0.15),
                          random_state=42):
    """
    Prepares a SAR image dataset for model training by extracting image chips
    based on a GeoDataFrame of geometries, normalizing them, and splitting
    them into train, validation, and test sets.

    **This version filters the GeoDataFrame to only include geometries
    fully contained within the raster bounds.**

    Args:
        gdf (gpd.GeoDataFrame): A GeoDataFrame containing polygon geometries and a 'label' column.
        raster_file_path (str): The file path to the source raster image (e.g., a GeoTIFF).
        output_dir (str): The root directory where the dataset splits will be saved.
        target_size (tuple, optional): The (height, width) for the final padded images. Defaults to (256, 256).
        split_ratios (tuple, optional): A tuple (train, val, test) specifying the
                                        proportions for each set. Must sum to 1.0.
                                        Defaults to (0.7, 0.15, 0.15).
        random_state (int, optional): Seed for the random number generator for reproducibility. Defaults to 42.
    """
    print("--- Starting Dataset Preparation ---")

    # --- 1. Create Output Directories ---
    print(f"Creating directory structure in '{output_dir}'...")
    # Ensure labels are strings for directory paths
    unique_labels = gdf['label'].unique().astype(str)
    for split in ['train', 'val', 'test']:
        for label in unique_labels:
            os.makedirs(os.path.join(output_dir, split, label), exist_ok=True)

    # --- 2. Open Raster, Filter, and Split Data ---
    try:
        with rasterio.open(raster_file_path) as src:

            # --- START: MODIFIED/ADDED SECTION (FILTERING) ---

            # Get the raster's bounds as a shapely polygon
            raster_bounds_poly = box(*src.bounds)

            # Ensure the GeoDataFrame's CRS matches the raster's CRS
            if gdf.crs != src.crs:
                print(f"Reprojecting GeoDataFrame from {gdf.crs} to {src.crs}...")
                gdf = gdf.to_crs(src.crs)

            # Filter the GDF to include only polygons fully WITHIN the raster bounds
            initial_count = len(gdf)
            gdf = gdf[gdf.geometry.within(raster_bounds_poly)].copy()
            filtered_count = len(gdf)

            if filtered_count < initial_count:
                print(f"Filtered out {initial_count - filtered_count} footprints that were not fully within the raster bounds.")

            if filtered_count == 0:
                print("Error: No footprints are fully within the raster bounds. Stopping.")
                return

            print(f"Proceeding with {filtered_count} valid footprints.")
            # --- END: MODIFIED/ADDED SECTION (FILTERING) ---


            # --- 3. Stratified Train/Val/Test Split ---

            train_prop, val_prop, test_prop = split_ratios

            # Input validation for ratios
            if not np.isclose(sum(split_ratios), 1.0):
                raise ValueError(f"split_ratios must sum to 1.0, but got {sum(split_ratios)}")
            if not all(0.0 <= prop <= 1.0 for prop in split_ratios):
                raise ValueError("All values in split_ratios must be between 0.0 and 1.0")

            # Step 1: Split into (train + val) and test sets
            train_val_gdf, test_gdf = _safe_stratified_split(
                gdf,
                test_size=test_prop,
                random_state=random_state
            )

            # Step 2: Split the (train + val) set into train and val
            train_plus_val_prop = train_prop + val_prop

            if train_plus_val_prop > 0:
                val_split_prop_of_remainder = val_prop / train_plus_val_prop
            else:
                val_split_prop_of_remainder = 0.0

            train_gdf, val_gdf = _safe_stratified_split(
                train_val_gdf,
                test_size=val_split_prop_of_remainder,
                random_state=random_state
            )

            print("\nDataset split into:")
            print(f" - Training set:   {len(train_gdf)} samples")
            print(f" - Validation set: {len(val_gdf)} samples")
            print(f" - Test set:       {len(test_gdf)} samples\n")

            # --- 4. Process and Save Images for Each Split ---
            # Check if GDFs are empty before processing
            if not train_gdf.empty:
                _process_and_save(train_gdf, 'train', src, output_dir, target_size)
            if not val_gdf.empty:
                _process_and_save(val_gdf, 'val', src, output_dir, target_size)
            if not test_gdf.empty:
                _process_and_save(test_gdf, 'test', src, output_dir, target_size)

    except RasterioIOError:
        print(f"Error: Could not open the file at '{raster_file_path}'.")
        return
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        return

    print("\n--- Dataset preparation complete. ---")

def _copy_contents(source_dir, dest_dir):
    """
    Helper function to copy contents from a single source to a destination,
    handling file name conflicts by renaming.
    """
    print(f"\n--- Copying from source: '{source_dir}' ---")
    for root, dirs, files in os.walk(source_dir):
        # Determine the corresponding path in the destination directory
        relative_path = os.path.relpath(root, source_dir)
        dest_path_dir = os.path.join(dest_dir, relative_path)

        for filename in files:
            source_filepath = os.path.join(root, filename)
            dest_filepath = os.path.join(dest_path_dir, filename)

            # --- Handle potential file name conflicts ---
            if os.path.exists(dest_filepath):
                base, extension = os.path.splitext(filename)
                counter = 1
                # Find a new, unique name by adding a suffix
                while os.path.exists(dest_filepath):
                    new_filename = f"{base}_merged_{counter}{extension}"
                    dest_filepath = os.path.join(dest_path_dir, new_filename)
                    counter += 1
                print(f"  - CONFLICT: '{filename}' exists. Renaming to '{new_filename}'.")

            # Copy the file to the new destination
            # shutil.copy2 preserves file metadata (like creation/modification time)
            shutil.copy2(source_filepath, dest_filepath)
            print(f"  - COPIED: '{filename}' to '{dest_path_dir}'")

def merge_and_copy_directories(dir1, dir2, new_dir):
    """
    Merges two identically structured directories into a new directory.

    It creates a new directory, copies the contents of the first directory,
    and then copies the contents of the second, handling file name conflicts.

    Args:
        dir1 (str): Path to the first source directory.
        dir2 (str): Path to the second source directory.
        new_dir (str): Path to the new destination directory to be created.
    """
    print("Starting copy and merge process...")
    if os.path.exists(new_dir):
        print(f"Error: Destination directory '{new_dir}' already exists. Please choose a new path.")
        return

    # --- 1. Create the new directory structure ---
    print(f"Creating new directory structure at '{new_dir}'...")
    # Walk through one of the source directories to replicate its structure
    for root, dirs, files in os.walk(dir1):
        relative_path = os.path.relpath(root, dir1)
        dest_path_dir = os.path.join(new_dir, relative_path)
        os.makedirs(dest_path_dir, exist_ok=True)

    # --- 2. Copy contents from both directories ---
    _copy_contents(dir1, new_dir)
    _copy_contents(dir2, new_dir)

    print("\nMerge and copy complete! New directory is ready.")

def download_s3_file(url: str):
    """
    Downloads a file from a given URL and saves it using the
    filename extracted from the URL.
    """
    try:
        # Extract the filename from the last part of the URL
        filename = url.split('/')[-1]

        if not filename:
            print("Error: Could not determine filename from URL.")
            return

        print(f"Downloading '{filename}' from {url}...")

        # Send a GET request to the URL
        response = requests.get(url)

        # Check if the request was successful
        if response.status_code == 200:
            # Open the file in binary write mode and save the content
            with open(filename, 'wb') as f:
                f.write(response.content)
            print(f"Successfully downloaded and saved as '{filename}'.")
        else:
            # Print an error message if the download failed
            print(f"Failed to download file. Status code: {response.status_code}")

    except requests.exceptions.RequestException as e:
        # Handle potential network or request errors
        print(f"An error occurred during download: {e}")
    except Exception as e:
        # Handle other potential errors (e.g., file writing)
        print(f"An unexpected error occurred: {e}")

import requests
import shutil
import os
import urllib.parse

def download_geotiff(url, output_dir=None):
    """
    Downloads a file from a URL, saving it locally.
    It streams the download, making it suitable for large files.

    Args:
        url (str): The URL of the GeoTIFF file to download.
        output_dir (str, optional): The local directory to save the file in.
                                  If None, saves to the current working directory.

    Returns:
        str: The original filename (e.g., "my_image.tif") if successful,
             None otherwise.
    """
    try:
        # --- 1. Parse the URL to get the original filename ---
        # This gets the path component (e.g., "/path/to/my_image.tif")
        path = urllib.parse.urlparse(url).path

        # This extracts the base filename (e.g., "my_image.tif")
        original_filename = os.path.basename(path)

        # Handle edge case where URL might not have a filename
        if not original_filename:
            print(f"Error: Could not determine a filename from the URL: {url}")
            return None

        # --- 2. Determine the full local save path ---
        if output_dir:
            # Create the output directory if it doesn't exist
            os.makedirs(output_dir, exist_ok=True)
            local_save_path = os.path.join(output_dir, original_filename)
        else:
            local_save_path = original_filename

        # --- 3. Download the file using a streaming request ---
        print(f"Downloading '{original_filename}' from {url}...")

        # 'stream=True' is crucial for large files
        with requests.get(url, stream=True) as response:
            # Check if the request was successful (e.g., 200 OK)
            response.raise_for_status()  # Will raise an error for 4xx or 5xx

            # Open the local file and write the content in chunks
            with open(local_save_path, 'wb') as f:
                # shutil.copyfileobj is an efficient way to stream data
                shutil.copyfileobj(response.raw, f)

        print(f"Successfully downloaded and saved to '{local_save_path}'.")

        # --- 4. Return the original filename as requested ---
        return original_filename

    except requests.exceptions.HTTPError as http_err:
        print(f"HTTP error occurred: {http_err} - Check the URL.")
    except requests.exceptions.ConnectionError as conn_err:
        print(f"Connection error occurred: {conn_err}")
    except requests.exceptions.RequestException as e:
        print(f"An error occurred during download: {e}")
    except IOError as e:
        print(f"Error writing file to disk: {e}")

    return None


def unzip_file(zip_path, extract_to_path):
    """
    Unzips a specified zip file to a target directory.

    Args:
        zip_path (str): The path to the zip file.
        extract_to_path (str): The path to the directory where contents will be extracted.
    """
    try:
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            zip_ref.extractall(extract_to_path)
        print(f"Successfully unzipped '{zip_path}' to '{extract_to_path}'.")
    except zipfile.BadZipFile:
        print(f"Error: '{zip_path}' is not a valid zip file.")
    except FileNotFoundError:
        print(f"Error: Zip file not found at '{zip_path}'.")
    except Exception as e:
        print(f"An unexpected error occurred: {e}")

import geopandas as gpd
from shapely.geometry import Polygon

def query_gdb_contained_by_polygon(gdb_path, layer_name, polygon_gdf):
    """
    Queries a layer in a File Geodatabase (GDB) and returns rows fully
    contained within an input polygon.

    Args:
        gdb_path (str): Path to the File Geodatabase (.gdb).
        layer_name (str): Name of the layer to query.
        polygon_gdf (gpd.GeoDataFrame): A GeoDataFrame with a single polygon geometry.

    Returns:
        gpd.GeoDataFrame: A GeoDataFrame containing rows from the GDB layer
                          that are fully contained within the polygon.
    """
    # Read the layer from the GDB into a GeoDataFrame
    gdf = gpd.read_file(gdb_path, layer=layer_name)

    # Ensure the polygon_gdf has only one geometry
    if len(polygon_gdf) != 1:
        raise ValueError("The input polygon_gdf must contain exactly one polygon.")

    # Get the polygon geometry
    polygon_geom = polygon_gdf.geometry.iloc[0]

    # Reproject the GDB layer's CRS to match the polygon's CRS, if they don't match
    if gdf.crs != polygon_gdf.crs:
        gdf = gdf.to_crs(polygon_gdf.crs)

    # Filter the GeoDataFrame to find features that are completely within the polygon
    contained_gdf = gdf[gdf.geometry.within(polygon_geom)]

    return contained_gdf

import rasterio
import numpy as np
import matplotlib.pyplot as plt
from rasterio.mask import mask
import geopandas as gpd # Assuming geopandas is used

def filter_polygons_by_aoi_and_plot(image_path, aoi_gdf, polygons_gdf):
    """
    Crops a raster to an AOI, filters a polygon GeoDataFrame to include
    only polygons fully within that AOI, plots the result, and returns
    the filtered GeoDataFrame.

    Args:
        image_path (str): Path to the raster image file.
        aoi_gdf (gpd.GeoDataFrame): GeoDataFrame containing the AOI geometry.
        polygons_gdf (gpd.GeoDataFrame): GeoDataFrame of polygons to be filtered.

    Returns:
        gpd.GeoDataFrame: A GeoDataFrame containing only the polygons that
                          are fully within the AOI. Returns None if an
                          error occurs (e.g., file not found).
    """

    try:
        with rasterio.open(image_path) as src:
            print(f"Image has {src.count} channels/bands.")

            # Ensure local copies are modified, not the originals outside the function
            aoi_gdf_proj = aoi_gdf.copy()
            polygons_gdf_proj = polygons_gdf.copy()

            # Reproject both GDFs if their CRS differs from the raster
            if aoi_gdf_proj.crs != src.crs:
                aoi_gdf_proj = aoi_gdf_proj.to_crs(src.crs)
            if polygons_gdf_proj.crs != src.crs:
                polygons_gdf_proj = polygons_gdf_proj.to_crs(src.crs)

            # --- Filtering Logic ---
            # Create a single dissolved boundary from the AOI to test against
            aoi_boundary = aoi_gdf_proj.unary_union

            # Use .within() to select only polygons *fully inside* the AOI boundary
            gdf_fully_inside = polygons_gdf_proj[polygons_gdf_proj.within(aoi_boundary)]

            print(f"Found {len(polygons_gdf_proj)} intersecting polygons (original).")
            print(f"Found {len(gdf_fully_inside)} *fully contained* polygons.")
            # --- End Filtering Logic ---

            # Get geometries for masking the raster
            aoi_geom = [geom for geom in aoi_gdf_proj.geometry]

            cropped_image, cropped_transform = mask(src, aoi_geom, crop=True)
            nodata_val = src.nodata or 0

        # --- 5. Select and Normalize the Band for Visualization ---
        first_band = cropped_image[0]
        valid_data_mask = first_band != nodata_val

        # Check if there is any valid data to plot
        if not np.any(valid_data_mask):
            print("Warning: No valid data found in the cropped raster. Plotting may fail.")
            # Still return the filtered geometries
            return gdf_fully_inside

        vmin, vmax = np.percentile(first_band[valid_data_mask], (2, 98))

        # Avoid division by zero if vmin and vmax are the same
        if vmin == vmax:
            clipped_band = np.clip(first_band, vmin, vmax)
            normalized_band = (clipped_band - vmin) / 1.0  # Avoid division by zero, result is 0
        else:
            clipped_band = np.clip(first_band, vmin, vmax)
            normalized_band = (clipped_band - vmin) / (vmax - vmin)


        # --- 6. Plot the Raster and Overlay the *filtered* 'gdf_fully_inside' ---
        fig, ax = plt.subplots(figsize=(10, 10))

        extent = [cropped_transform[2], cropped_transform[2] + cropped_transform[0] * cropped_image.shape[2],
                  cropped_transform[5] + cropped_transform[4] * cropped_image.shape[1], cropped_transform[5]]

        ax.imshow(normalized_band, cmap='gray', extent=extent)

        # Overlay the new 'gdf_fully_inside'
        if not gdf_fully_inside.empty:
            gdf_fully_inside.plot(ax=ax, facecolor='none', edgecolor='yellow', linewidth=1.5)

        ax.set_title("Normalized Capella image with Building Footprints")
        ax.set_xlabel("Longitude")
        ax.set_ylabel("Latitude")
        plt.show()

        # --- Return the filtered GeoDataFrame ---
        return gdf_fully_inside

    except rasterio.errors.RasterioIOError:
        print(f"Error: Could not open the file at '{image_path}'.")
        print("Please make sure the file path is correct and the file exists.")
        return None
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        return None

"""***Create Palisades training dataset:***"""



# Building Damage Dataset 1 (labeled Maxar data):

import geopandas as gpd
import fiona

# URL to a sample GeoPackage file
url = "https://data.humdata.org/dataset/30768ff0-289b-4fda-96d9-7209243c984d/resource/9650c5fe-c29b-429e-81e3-537688a74f60/download/maxar_palisades_1050010040277500_damage_predictions.gpkg"

# 1. (Optional) List the layers available at the URL
try:
    layer_names = fiona.listlayers(url)
    print(f"Layers found at URL: {layer_names}")
except Exception as e:
    print(f"Could not list layers: {e}")
    # Assign a layer name if you know it
    layer_names = ['geopackage_srs']


# 2. Read a specific layer directly from the URL
if layer_names:
    gdf_mx = gpd.read_file(url, layer=layer_names[0])

    # Display the first few rows to verify
    print("\nGeoDataFrame loaded successfully:")

# # If the URL does not work download locally and run:
# layer_names = fiona.listlayers('/content/maxar_palisades_1050010040277500_damage_predictions.gpkg')
# gdf_mx = gpd.read_file('/content/maxar_palisades_1050010040277500_damage_predictions.gpkg', layer=layer_names[0])

# # Display the first few rows to verify
# print("\nGeoDataFrame loaded successfully:")

# gdf_mx.explore()

# Define AOI:

# 1. Merge all polygons into a single MultiPolygon geometry
all_polygons_merged = gdf_mx.unary_union

# 2. Get the envelope (bounding box) of the merged geometry
bounding_box = all_polygons_merged.envelope

# 3. Create the new GeoDataFrame 'gdf_aoi'
# We put the bounding_box in a list because GeoDataFrame.from_features
# expects an iterable of geometries.
gdf_aoi = gpd.GeoDataFrame.from_features(
    [{"geometry": bounding_box, "properties": {"name": "my_aoi"}}],
    crs=gdf_mx.crs  # Ensure the CRS is carried over
)

# gdf_aoi.explore()

# Create label column:
gdf_mx['label'] = gdf_mx['damaged']

# Buffer:

# Change to projected CRS:
gdf_mx = gdf_mx.to_crs("EPSG:32611")

# Apply a 3-meter buffer around each polygon
gdf_mx['geometry'] = gdf_mx.buffer(5)

gdf_mx['label'].value_counts()

import uuid

# Create UUIDs and set as index
uuids = [str(uuid.uuid4()) for _ in range(len(gdf_mx))]
gdf_mx.index = uuids
gdf_mx.index.name = 'uuid'

# Or reset to column
gdf_mx = gdf_mx[['geometry', 'label']]
gdf_mx.head()

# Download image:

f1 = download_geotiff(url='https://capella-open-data.s3.amazonaws.com/data/2025/1/11/CAPELLA_C14_SS_GEO_HH_20250111163649_20250111163705/CAPELLA_C14_SS_GEO_HH_20250111163649_20250111163705.tif')
f1

# --- 3. Specify the path to your actual SAR image ---
file_path = f1

gdf_intersecting = gdf_mx

# --- 4. Open the image, check bands, and crop using gdf_roi ---
try:
    with rasterio.open(file_path) as src:
        print(f"Image has {src.count} channels/bands.")

        # Reproject both GDFs if their CRS differs from the raster
        if gdf_aoi.crs != src.crs:
            gdf_aoi = gdf_aoi.to_crs(src.crs)
        if gdf_intersecting.crs != src.crs:
            # Corrected this line (was gdf_mx)
            gdf_intersecting = gdf_intersecting.to_crs(src.crs)

        # --- MODIFICATION START ---
        # Create a single dissolved boundary from the AOI to test against
        aoi_boundary = gdf_aoi.unary_union

        # Use .within() to select only polygons *fully inside* the AOI boundary
        gdf_fully_inside = gdf_intersecting[gdf_intersecting.within(aoi_boundary)]

        print(f"Found {len(gdf_intersecting)} intersecting polygons.")
        print(f"Found {len(gdf_fully_inside)} *fully contained* polygons.")
        # --- MODIFICATION END ---

        # Get geometries for masking the raster (this part is unchanged)
        aoi_geom = [geom for geom in gdf_aoi.geometry]

        cropped_image, cropped_transform = mask(src, aoi_geom, crop=True)
        nodata_val = src.nodata or 0

    # --- 5. Select and Normalize the Band for Visualization ---
    first_band = cropped_image[0]
    valid_data_mask = first_band != nodata_val
    vmin, vmax = np.percentile(first_band[valid_data_mask], (2, 98))
    clipped_band = np.clip(first_band, vmin, vmax)
    normalized_band = (clipped_band - vmin) / (vmax - vmin)

    # --- 6. Plot the Raster and Overlay the *filtered* 'gdf_fully_inside' ---
    fig, ax = plt.subplots(figsize=(10, 10))

    extent = [cropped_transform[2], cropped_transform[2] + cropped_transform[0] * cropped_image.shape[2],
              cropped_transform[5] + cropped_transform[4] * cropped_image.shape[1], cropped_transform[5]]

    ax.imshow(normalized_band, cmap='gray', extent=extent)

    # --- MODIFIED PLOT ---
    # Overlay the new 'gdf_fully_inside'
    gdf_fully_inside.plot(ax=ax, facecolor='none', edgecolor='yellow', linewidth=1.5)

    ax.set_title("Normalized Capella image with Building Footprints")
    ax.set_xlabel("Longitude")
    ax.set_ylabel("Latitude")
    plt.show()

except rasterio.errors.RasterioIOError:
    print(f"Error: Could not open the file at '{file_path}'.")
    print("Please make sure the file path is correct and the file exists.")

# # Remove existing dataset if needed:

# import shutil
# shutil.rmtree('palisades_building_dataset')

# Process dataset 1:
RASTER_PATH = f1
OUTPUT_DATASET_DIR = 'palisades_building_dataset'

# 3. Call the main function
# Note: This will fail if the dummy polygons are outside the raster's bounds,
# but it demonstrates how to call the function.
print("Preparing SAR dataset...")
# 3. Call the main function
prepare_sar_dataset(gdf_fully_inside,
                    RASTER_PATH,
                    OUTPUT_DATASET_DIR,
                    target_size=(224, 224),
                    split_ratios=(0.7, 0.15, 0.15),
                    random_state=42)





"""***Create Lahaina training dataset:***"""

# The base URL for the feature layer's query endpoint
# Note: I'm using layer '2' from your example URL
base_url = "https://services1.arcgis.com/uujCiiEZAflDbdxE/arcgis/rest/services/InferencedBuildingDamage/FeatureServer/2/query"

# Step 1: Find out the total number of features
# We can do this by making a request with 'returnCountOnly=true'
count_params = {'where': '1=1', 'returnCountOnly': 'true', 'f': 'json'}
r = requests.get(base_url, params=count_params)
r.raise_for_status() # Raise an exception for bad status codes
total_features = r.json()['count']
print(f"Total features to download: {total_features}")

# Step 2: Loop through the data in pages
gdf_list = []
offset = 0
page_size = 2000  # The max record count for this server

while offset < total_features:
    print(f"Fetching features from {offset} to {offset + page_size}...")

    # Parameters for fetching a 'page' of GeoJSON data
    query_params = {
        'where': '1=1',
        'outFields': '*',      # Get all fields
        'f': 'geojson',        # Request GeoJSON format
        'resultOffset': offset,
        'resultRecordCount': page_size
    }

    # Construct the full URL with query parameters
    page_url = f"{base_url}?{'&'.join([f'{k}={v}' for k, v in query_params.items()])}"

    # Read the page into a GeoDataFrame
    page_gdf = gpd.read_file(page_url)

    # If the page is empty for some reason, break the loop
    if page_gdf.empty:
        break

    # Add the page's data to our list
    gdf_list.append(page_gdf)

    # Increment the offset for the next page
    offset += len(page_gdf)

# Step 3: Combine all the paged GeoDataFrames into one
if gdf_list:
    gdf_bldg = gpd.pd.concat(gdf_list, ignore_index=True)
    print(f"\nâœ… Success! Downloaded a total of {len(gdf_bldg)} features.")
    print("\nFirst 5 rows of the combined GeoDataFrame:")
    print(gdf_bldg.head())
else:
    print("No features were downloaded.")

# Buffer:

# Change to projected CRS:
gdf_bldg = gdf_bldg.to_crs("EPSG:32604")

# Apply a 3-meter buffer around each polygon
gdf_bldg['geometry'] = gdf_bldg.buffer(5)

# Create label column:
# Map label to binary (1/0):
gdf_bldg['label'] = gdf_bldg['ClassLabel'].map({'Damaged': 1, 'Undamaged': 0})

# Create UUIDs and set as index
uuids = [str(uuid.uuid4()) for _ in range(len(gdf_bldg))]
gdf_bldg.index = uuids
gdf_bldg.index.name = 'uuid'

gdf_bldg = gdf_bldg[['geometry', 'label']]
gdf_bldg.head()

# Download image:

f2 = download_geotiff(url='https://capella-open-data.s3.amazonaws.com/data/2023/8/12/CAPELLA_C06_SP_GEO_HH_20230812045610_20230812045634/CAPELLA_C06_SP_GEO_HH_20230812045610_20230812045634.tif')
f2

# Define AOI:

# 1. Merge all polygons into a single MultiPolygon geometry
all_polygons_merged = gdf_bldg.unary_union

# 2. Get the envelope (bounding box) of the merged geometry
bounding_box = all_polygons_merged.envelope

# 3. Create the new GeoDataFrame 'gdf_aoi'
# We put the bounding_box in a list because GeoDataFrame.from_features
# expects an iterable of geometries.
gdf_aoi = gpd.GeoDataFrame.from_features(
    [{"geometry": bounding_box, "properties": {"name": "my_aoi"}}],
    crs=gdf_bldg.crs  # Ensure the CRS is carried over
)

# --- 3. Specify the path to your actual SAR image ---
file_path = f2

gdf_intersecting = gdf_bldg

# --- 4. Open the image, check bands, and crop using gdf_roi ---
try:
    with rasterio.open(file_path) as src:
        print(f"Image has {src.count} channels/bands.")

        # Reproject both GDFs if their CRS differs from the raster
        if gdf_aoi.crs != src.crs:
            gdf_aoi = gdf_aoi.to_crs(src.crs)
        if gdf_intersecting.crs != src.crs:
            # Corrected this line (was gdf_mx)
            gdf_intersecting = gdf_intersecting.to_crs(src.crs)

        # --- MODIFICATION START ---
        # Create a single dissolved boundary from the AOI to test against
        aoi_boundary = gdf_aoi.unary_union

        # Use .within() to select only polygons *fully inside* the AOI boundary
        gdf_fully_inside = gdf_intersecting[gdf_intersecting.within(aoi_boundary)]

        print(f"Found {len(gdf_intersecting)} intersecting polygons.")
        print(f"Found {len(gdf_fully_inside)} *fully contained* polygons.")
        # --- MODIFICATION END ---

        # Get geometries for masking the raster (this part is unchanged)
        aoi_geom = [geom for geom in gdf_aoi.geometry]

        cropped_image, cropped_transform = mask(src, aoi_geom, crop=True)
        nodata_val = src.nodata or 0

    # --- 5. Select and Normalize the Band for Visualization ---
    first_band = cropped_image[0]
    valid_data_mask = first_band != nodata_val
    vmin, vmax = np.percentile(first_band[valid_data_mask], (2, 98))
    clipped_band = np.clip(first_band, vmin, vmax)
    normalized_band = (clipped_band - vmin) / (vmax - vmin)

    # --- 6. Plot the Raster and Overlay the *filtered* 'gdf_fully_inside' ---
    fig, ax = plt.subplots(figsize=(10, 10))

    extent = [cropped_transform[2], cropped_transform[2] + cropped_transform[0] * cropped_image.shape[2],
              cropped_transform[5] + cropped_transform[4] * cropped_image.shape[1], cropped_transform[5]]

    ax.imshow(normalized_band, cmap='gray', extent=extent)

    # --- MODIFIED PLOT ---
    # Overlay the new 'gdf_fully_inside'
    gdf_fully_inside.plot(ax=ax, facecolor='none', edgecolor='yellow', linewidth=1.5)

    ax.set_title("Normalized Capella image with Building Footprints")
    ax.set_xlabel("Longitude")
    ax.set_ylabel("Latitude")
    plt.show()

except rasterio.errors.RasterioIOError:
    print(f"Error: Could not open the file at '{file_path}'.")
    print("Please make sure the file path is correct and the file exists.")

# # Remove existing dataset if needed:

# import shutil
# shutil.rmtree('lahaina_building_dataset')

# Dataset 2:

RASTER_PATH = f2
OUTPUT_DATASET_DIR = 'lahaina_building_dataset'

# 3. Call the main function
# Note: This will fail if the dummy polygons are outside the raster's bounds,
# but it demonstrates how to call the function.
print("Preparing SAR dataset...")
prepare_sar_dataset(gdf_fully_inside,
                    RASTER_PATH,
                    OUTPUT_DATASET_DIR,
                    target_size=(224, 224),
                    split_ratios=(0.7, 0.15, 0.15),
                    random_state=42)

"""***Merge datasets:***"""

# IMPORTANT: Set your directory paths here
# First source directory
DIRECTORY_ONE = 'palisades_building_dataset'

# Second source directory
DIRECTORY_TWO = 'lahaina_building_dataset'

# The new directory that will be created to hold the merged contents
NEW_MERGED_DIRECTORY = 'building_dataset'

# Check if paths are valid before running
if not os.path.isdir(DIRECTORY_ONE) or not os.path.isdir(DIRECTORY_TWO):
    print("Error: One or both of the specified source paths are not valid directories.")
    print("Please update the DIRECTORY_ONE and DIRECTORY_TWO variables.")
else:
    merge_and_copy_directories(DIRECTORY_ONE, DIRECTORY_TWO, NEW_MERGED_DIRECTORY)

import os
import yaml
from ultralytics import YOLO

# --- 1. Configuration ---
# Define paths for the dataset and the YAML file to be created
DATASET_DIR = 'building_dataset'
YAML_FILE_PATH = 'data.yaml' # This YAML file is not needed for classification training

# --- 2. Create YAML File Programmatically ---
# This section is not strictly needed for YOLO classification training,
# as the data argument expects a directory path.
# Keeping it for reference, but it won't be used by model.train().
print(f"Creating '{YAML_FILE_PATH}' file (Note: Not used directly by YOLO classification trainer)...")

# Discover class names by reading the subdirectories in the 'train' folder
train_path = os.path.join(DATASET_DIR, 'train')
class_names = sorted(os.listdir(train_path))
num_classes = len(class_names)

# Create the dictionary with the required data
yaml_data = {
    'train': os.path.join(DATASET_DIR, 'train'),
    'val': os.path.join(DATASET_DIR, 'val'),
    'test': os.path.join(DATASET_DIR, 'test'),
    'nc': num_classes,
    'names': class_names
}

# Write the dictionary to the YAML file
with open(YAML_FILE_PATH, 'w') as f:
    yaml.dump(yaml_data, f, default_flow_style=False)

print("YAML file created successfully.")
print(f"  - Classes found: {class_names}")


# --- 3. Train the YOLO Classifier ---
# Load a pre-trained YOLOv8 classification model
model = YOLO('yolov8l-cls.pt')

# Train the model using the path to the dataset directory
results = model.train(
    data=DATASET_DIR,    # Provide the path to the dataset directory
    epochs=50,
    imgsz=256,
    patience=10,
    batch=32,
    name='building_damage_classifier'
)

print("\nTraining complete. Model and results are saved in the 'runs' directory.")

# Save runs data to Google Drive:
# drive.mount('/content/drive')

# !cp -r /content/runs "/content/drive/MyDrive/yolo_sar_building_damage_detection"

from IPython.display import Image as i

# Replace 'path/to/your/image.png' with the actual path to your image file
i('/content/runs/classify/building_damage_classifier/confusion_matrix_normalized.png')

metrics = model.val(split='test', data='data.yaml')

# --- Get Accuracy ---
# The metrics object for classifiers directly provides top-1 and top-5 accuracy
top1_accuracy = metrics.top1
top5_accuracy = metrics.top5

print(f"Top-1 Accuracy: {top1_accuracy:.4f}")
print(f"Top-5 Accuracy: {top5_accuracy:.4f}")

# --- Calculate Precision, Recall, and F1-Score ---
# These are derived from the confusion matrix, which is also in the metrics object
# Access the internal matrix data from the ConfusionMatrix object
conf_matrix_array = metrics.confusion_matrix.matrix

conf_matrix_array = conf_matrix_array[:2, :2]

# True Positives (TP) are on the diagonal
tp = np.diag(conf_matrix_array)

# False Positives (FP) are the sum of each column, minus the TP
fp = np.sum(conf_matrix_array, axis=0) - tp

# False Negatives (FN) are the sum of each row, minus the TP
fn = np.sum(conf_matrix_array, axis=1) - tp

# --- Per-Class Metrics ---
# Add a small epsilon to avoid division by zero
epsilon = 1e-7

per_class_precision = tp / (tp + fp + epsilon)
per_class_recall = tp / (tp + fn + epsilon)
per_class_f1 = 2 * (per_class_precision * per_class_recall) / (per_class_precision + per_class_recall + epsilon)

print("\nPer-Class Metrics:")
for i, class_name in enumerate(model.names.values()):
    print(f"  Class: {class_name}")
    print(f"    Precision: {per_class_precision[i]:.4f}")
    print(f"    Recall:    {per_class_recall[i]:.4f}")
    print(f"    F1-Score:  {per_class_f1[i]:.4f}")

# --- Macro Averages (Overall Performance) ---
macro_precision = np.mean(per_class_precision)
macro_recall = np.mean(per_class_recall)
macro_f1 = np.mean(per_class_f1)

print("\nOverall (Macro Average) Metrics:")
print(f"Precision: {macro_precision:.4f}")
print(f"Recall:    {macro_recall:.4f}")
print(f"F1-Score:  {macro_f1:.4f}")

"""***Test on different Palisades image (different collection angle):***"""

# Download image:

f3 = download_geotiff(url='https://capella-open-data.s3.amazonaws.com/data/2025/1/18/CAPELLA_C13_SM_GEO_HH_20250118200502_20250118200506/CAPELLA_C13_SM_GEO_HH_20250118200502_20250118200506.tif')
f3

# Define AOI:

# 1. Merge all polygons into a single MultiPolygon geometry
all_polygons_merged = gdf_mx.unary_union

# 2. Get the envelope (bounding box) of the merged geometry
bounding_box = all_polygons_merged.envelope

# 3. Create the new GeoDataFrame 'gdf_aoi'
# We put the bounding_box in a list because GeoDataFrame.from_features
# expects an iterable of geometries.
gdf_aoi = gpd.GeoDataFrame.from_features(
    [{"geometry": bounding_box, "properties": {"name": "my_aoi"}}],
    crs=gdf_mx.crs  # Ensure the CRS is carried over
)

import rasterio
import geopandas as gpd
import numpy as np
import matplotlib.pyplot as plt
from rasterio.mask import mask
from rasterio.features import shapes
from shapely.geometry import shape
from shapely.ops import unary_union

# --- 3. Specify the path to your actual SAR image ---
file_path = f3
gdf_intersecting = gdf_mx

# --- 4. Open the image, check bands, and crop using gdf_roi ---
try:
    with rasterio.open(file_path) as src:
        print(f"Image has {src.count} channels/bands.")

        # Reproject both GDFs if their CRS differs from the raster
        if gdf_aoi.crs != src.crs:
            gdf_aoi = gdf_aoi.to_crs(src.crs)
        if gdf_intersecting.crs != src.crs:
            gdf_intersecting = gdf_intersecting.to_crs(src.crs)

        # --- NEW: Get the geometry of the valid data pixels ---
        # Read the 'nodata' mask (band 1)
        # 255 means valid data, 0 means nodata
        valid_data_mask_array = src.read_masks(1)

        # Find all shapes (polygons) in the mask that are valid data (value=255)
        # We pass src.transform so the geometries are in the raster's CRS
        valid_data_geoms = list(shapes(
            valid_data_mask_array,
            mask=(valid_data_mask_array == 255),  # Only find shapes where mask is 255
            transform=src.transform
        ))

        # Convert the generated geometries into a single dissolved Shapely polygon
        if not valid_data_geoms:
            raise ValueError("No valid data found in the raster.")

        # valid_data_geoms is a list of (geometry_dict, value) tuples
        valid_data_polygons = [shape(geom) for geom, val in valid_data_geoms]
        valid_data_boundary = unary_union(valid_data_polygons)
        # --- END NEW ---

        # --- MODIFICATION START ---
        # Create a single dissolved boundary from the AOI to test against
        aoi_boundary = gdf_aoi.unary_union

        # --- NEW: Create the final boundary ---
        # The true processing area is the INTERSECTION of
        # where the AOI is and where the raster has valid data.
        final_processing_boundary = aoi_boundary.intersection(valid_data_boundary)

        if final_processing_boundary.is_empty:
            print("Warning: Your AOI and the valid image data do not overlap.")
            # Create an empty GDF to avoid errors later
            gdf_fully_inside = gdf_intersecting[gdf_intersecting.is_empty]
        else:
            # --- MODIFIED: Use the new final_processing_boundary ---
            # Use .within() to select only polygons *fully inside* this new combined boundary
            gdf_fully_inside = gdf_intersecting[gdf_intersecting.within(final_processing_boundary)]

        print(f"Found {len(gdf_intersecting)} intersecting polygons.")
        print(f"Found {len(gdf_fully_inside)} *fully contained* polygons (within AOI & valid data).")
        # --- MODIFICATION END ---

        # Get geometries for masking the raster (this part is unchanged)
        # We still use the original aoi_geom to define the *visual* crop
        aoi_geom = [geom for geom in gdf_aoi.geometry]

        cropped_image, cropped_transform = mask(src, aoi_geom, crop=True)
        nodata_val = src.nodata or 0

    # --- 5. Select and Normalize the Band for Visualization ---
    first_band = cropped_image[0]
    valid_data_mask = first_band != nodata_val

    # Check if there are any valid pixels to normalize
    if np.any(valid_data_mask):
        vmin, vmax = np.percentile(first_band[valid_data_mask], (2, 98))
        clipped_band = np.clip(first_band, vmin, vmax)

        # Avoid division by zero if all valid pixels are the same value
        if vmax - vmin > 0:
            normalized_band = (clipped_band - vmin) / (vmax - vmin)
        else:
            normalized_band = clipped_band
    else:
        print("Warning: No valid data in the cropped AOI for plotting.")
        normalized_band = first_band # Plot the empty crop

    # --- 6. Plot the Raster and Overlay the *filtered* 'gdf_fully_inside' ---
    fig, ax = plt.subplots(figsize=(10, 10))

    extent = [cropped_transform[2], cropped_transform[2] + cropped_transform[0] * cropped_image.shape[2],
              cropped_transform[5] + cropped_transform[4] * cropped_image.shape[1], cropped_transform[5]]

    ax.imshow(normalized_band, cmap='gray', extent=extent)

    # --- MODIFIED PLOT ---
    # Overlay the new 'gdf_fully_inside'
    if not gdf_fully_inside.empty:
        gdf_fully_inside.plot(ax=ax, facecolor='none', edgecolor='yellow', linewidth=1.0)

    ax.set_title("Normalized Capella image with Building Footprints")
    ax.set_xlabel("Longitude")
    ax.set_ylabel("Latitude")
    plt.show()

except rasterio.errors.RasterioIOError:
    print(f"Error: Could not open the file at '{file_path}'.")
    print("Please make sure the file path is correct and the file exists.")
except ValueError as e:
    print(f"Error: {e}")

# Remove existing dataset if needed:

import shutil
shutil.rmtree('palisades_test_dataset')

RASTER_PATH = f3

OUTPUT_DATASET_DIR = 'palisades_test_dataset'

# 3. Call the main function
prepare_sar_dataset(gdf_fully_inside, RASTER_PATH, OUTPUT_DATASET_DIR, target_size=(224, 224),
                          split_ratios=(0.0, 0.0, 1.0),
                          random_state=42)

# Load pre-trained/fine-tuned YOLO model:
from ultralytics import YOLO

model = YOLO("runs/classify/building_damage_classifier/weights/best.pt")

# Inference:
metrics = model.val(split='test', data='/content/palisades_test_dataset/test', cache=False)

# Evaluate performance:
# --- Get Accuracy ---
# The metrics object for classifiers directly provides top-1 and top-5 accuracy
top1_accuracy = metrics.top1
top5_accuracy = metrics.top5

print(f"Top-1 Accuracy: {top1_accuracy:.4f}")
print(f"Top-5 Accuracy: {top5_accuracy:.4f}")

# --- Calculate Precision, Recall, and F1-Score ---
# These are derived from the confusion matrix, which is also in the metrics object
# Access the internal matrix data from the ConfusionMatrix object
conf_matrix_array = metrics.confusion_matrix.matrix

conf_matrix_array = conf_matrix_array[:2, :2]

# True Positives (TP) are on the diagonal
tp = np.diag(conf_matrix_array)

# False Positives (FP) are the sum of each column, minus the TP
fp = np.sum(conf_matrix_array, axis=0) - tp

# False Negatives (FN) are the sum of each row, minus the TP
fn = np.sum(conf_matrix_array, axis=1) - tp

# --- Per-Class Metrics ---
# Add a small epsilon to avoid division by zero
epsilon = 1e-7

per_class_precision = tp / (tp + fp + epsilon)
per_class_recall = tp / (tp + fn + epsilon)
per_class_f1 = 2 * (per_class_precision * per_class_recall) / (per_class_precision + per_class_recall + epsilon)

print("\nPer-Class Metrics:")
for i, class_name in enumerate(model.names.values()):
    print(f"  Class: {class_name}")
    print(f"    Precision: {per_class_precision[i]:.4f}")
    print(f"    Recall:    {per_class_recall[i]:.4f}")
    print(f"    F1-Score:  {per_class_f1[i]:.4f}")

# --- Macro Averages (Overall Performance) ---
macro_precision = np.mean(per_class_precision)
macro_recall = np.mean(per_class_recall)
macro_f1 = np.mean(per_class_f1)

print("\nOverall (Macro Average) Metrics:")
print(f"Precision: {macro_precision:.4f}")
print(f"Recall:    {macro_recall:.4f}")
print(f"F1-Score:  {macro_f1:.4f}")

# Run predictions on all test images
print("Running predictions...")
results_0 = model.predict('/content/palisades_test_dataset/test/0', verbose=False)
results_1 = model.predict('/content/palisades_test_dataset/test/1', verbose=False)

# 3. Loop through results and extract the required information
data_for_df = []

for grp in [results_0, results_1]:
  for result in grp:
      # Get the filename from the full path
      filename = os.path.basename(result.path)

      # Get the index of the top prediction
      predicted_class_idx = result.probs.top1

      # Get the class name using the index
      predicted_class_name = model.names[predicted_class_idx]

      # Append the data as a dictionary to our list
      data_for_df.append({
          'image_filename': filename,
          'predicted_label': predicted_class_name
      })

# 4. Create the pandas DataFrame
predictions_df = pd.DataFrame(data_for_df)

# Display the final DataFrame
print(predictions_df.head())

predictions_df['uuid'] = predictions_df['image_filename'].str.split('_', expand=True)[1].str.split('.', expand=True)[0]
predictions_df.head()

# Merge the predictions back to the gdf_intersecting GeoDataFrame
# Use a left merge to keep all geometries from gdf_intersecting and add prediction results
gdf_results = gdf_mx.merge(predictions_df, left_on='uuid', right_on='uuid', how='left')

# Drop the redundant 'bld_id' column from the merged dataframe
gdf_results = gdf_results.drop(columns=['uuid'])

# Display the first few rows of the merged GeoDataFrame
print(gdf_results.head())

gdf_results = gdf_results.dropna()

gdf_results['predicted_label'] = gdf_results['predicted_label'].astype('int64')

import geopandas as gpd
import pandas as pd
import numpy as np
from shapely.geometry import Polygon
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches

# 2. Create the 'outcome' column (same as before)
conditions = [
    (gdf_results['label'] == 0) & (gdf_results['predicted_label'] == 0),
    (gdf_results['label'] == 1) & (gdf_results['predicted_label'] == 0),
    (gdf_results['label'] == 1) & (gdf_results['predicted_label'] == 1),
    (gdf_results['label'] == 0) & (gdf_results['predicted_label'] == 1)
]
outcomes = [
    'Correctly Predicted 0 (TN)', 'Incorrectly Predicted 0 (FN)',
    'Correctly Predicted 1 (TP)', 'Incorrectly Predicted 1 (FP)'
]
gdf_results['outcome'] = np.select(conditions, outcomes, default='Unknown')

# 3. Define the Color Map (same as before)
color_map = {
    'Correctly Predicted 0 (TN)': 'green',
    'Incorrectly Predicted 0 (FN)': 'yellow',
    'Correctly Predicted 1 (TP)': 'red',
    'Incorrectly Predicted 1 (FP)': 'orange',
    'Unknown': 'gray'
}

# --- NEW: Manually create the color column ---
# Use the .map() method to apply the color dictionary to the 'outcome' column
gdf_results['plot_color'] = gdf_results['outcome'].map(color_map)

# --- MODIFIED .explore() call ---
# 4. Create the map using the new 'plot_color' column directly
m = gdf_results.explore(
    color=gdf_results['plot_color'], # Use the 'color' parameter, not 'column' and 'cmap'
    style_kwds={
        'fillOpacity': 0.1,
        'weight': 3
    }
)

# Display the map in a notebook
m



"""***Run on Colorado wildfire:***"""

# Load pre-trained/fine-tuned YOLO model:
from ultralytics import YOLO

model = YOLO("/content/drive/MyDrive/yolo_sar_building_damage_detection/runs/classify/building_damage_classifier/weights/best.pt")

# Download image:

f4 = download_geotiff(url='https://capella-open-data.s3.amazonaws.com/data/2021/12/31/CAPELLA_C03_SP_GEO_HH_20211231164052_20211231164115/CAPELLA_C03_SP_GEO_HH_20211231164052_20211231164115.tif')
f4

# USA Structures data:
url = 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Colorado/Deliverable20230630CO.zip'
download_file(url, 'co_struc.zip')

# Create the extraction directory if it doesn't exist
import os

if not os.path.exists('co_struc'):
    os.makedirs('co_struc')

unzip_file('co_struc.zip', 'co_struc')

gdb_path = 'co_struc/Deliverable20230630CO/CO_Structures.gdb'

# Draw a WKT polygon here: https://wktmap.com/
p = wkt.loads('POLYGON ((-105.190315 39.922639, -105.072212 39.922639, -105.072212 39.998953, -105.190315 39.998953, -105.190315 39.922639))')

# Create a GeoDataFrame for the polygon with a specified CRS
polygon_gdf = gpd.GeoDataFrame([1], geometry=[p], crs="EPSG:4326")

# Define the path to your GDB and the layer name
layer_name = "CO_Structures"

# Call the function to get the contained features
contained_features = query_gdb_contained_by_polygon(gdb_path, layer_name, polygon_gdf)
contained_features.head()

contained_features = contained_features.explode()

# Buffer:
gdf_co = contained_features.copy()

# Change to projected CRS:
gdf_co = gdf_co.to_crs("EPSG:32610")

# Apply a 5-meter buffer around each polygon
gdf_co['geometry'] = gdf_co.buffer(5)

# Define AOI:

# 1. Merge all polygons into a single MultiPolygon geometry
all_polygons_merged = gdf_co.unary_union

# 2. Get the envelope (bounding box) of the merged geometry
bounding_box = all_polygons_merged.envelope

# 3. Create the new GeoDataFrame 'gdf_aoi'
# We put the bounding_box in a list because GeoDataFrame.from_features
# expects an iterable of geometries.
gdf_aoi = gpd.GeoDataFrame.from_features(
    [{"geometry": bounding_box, "properties": {"name": "my_aoi"}}],
    crs=gdf_co.crs  # Ensure the CRS is carried over
)

import rasterio
import geopandas as gpd
import numpy as np
import matplotlib.pyplot as plt
from rasterio.mask import mask
from rasterio.features import shapes
from shapely.geometry import shape
from shapely.ops import unary_union

# --- 3. Specify the path to your actual SAR image ---
file_path = f4
gdf_intersecting = gdf_co

# --- 4. Open the image, check bands, and crop using gdf_roi ---
try:
    with rasterio.open(file_path) as src:
        print(f"Image has {src.count} channels/bands.")

        # Reproject both GDFs if their CRS differs from the raster
        if gdf_aoi.crs != src.crs:
            gdf_aoi = gdf_aoi.to_crs(src.crs)
        if gdf_intersecting.crs != src.crs:
            gdf_intersecting = gdf_intersecting.to_crs(src.crs)

        # --- NEW: Get the geometry of the valid data pixels ---
        # Read the 'nodata' mask (band 1)
        # 255 means valid data, 0 means nodata
        valid_data_mask_array = src.read_masks(1)

        # Find all shapes (polygons) in the mask that are valid data (value=255)
        # We pass src.transform so the geometries are in the raster's CRS
        valid_data_geoms = list(shapes(
            valid_data_mask_array,
            mask=(valid_data_mask_array == 255),  # Only find shapes where mask is 255
            transform=src.transform
        ))

        # Convert the generated geometries into a single dissolved Shapely polygon
        if not valid_data_geoms:
            raise ValueError("No valid data found in the raster.")

        # valid_data_geoms is a list of (geometry_dict, value) tuples
        valid_data_polygons = [shape(geom) for geom, val in valid_data_geoms]
        valid_data_boundary = unary_union(valid_data_polygons)
        # --- END NEW ---

        # --- MODIFICATION START ---
        # Create a single dissolved boundary from the AOI to test against
        aoi_boundary = gdf_aoi.unary_union

        # --- NEW: Create the final boundary ---
        # The true processing area is the INTERSECTION of
        # where the AOI is and where the raster has valid data.
        final_processing_boundary = aoi_boundary.intersection(valid_data_boundary)

        if final_processing_boundary.is_empty:
            print("Warning: Your AOI and the valid image data do not overlap.")
            # Create an empty GDF to avoid errors later
            gdf_fully_inside = gdf_intersecting[gdf_intersecting.is_empty]
        else:
            # --- MODIFIED: Use the new final_processing_boundary ---
            # Use .within() to select only polygons *fully inside* this new combined boundary
            gdf_fully_inside = gdf_intersecting[gdf_intersecting.within(final_processing_boundary)]

        print(f"Found {len(gdf_intersecting)} intersecting polygons.")
        print(f"Found {len(gdf_fully_inside)} *fully contained* polygons (within AOI & valid data).")
        # --- MODIFICATION END ---

        # Get geometries for masking the raster (this part is unchanged)
        # We still use the original aoi_geom to define the *visual* crop
        aoi_geom = [geom for geom in gdf_aoi.geometry]

        cropped_image, cropped_transform = mask(src, aoi_geom, crop=True)
        nodata_val = src.nodata or 0

    # --- 5. Select and Normalize the Band for Visualization ---
    first_band = cropped_image[0]
    valid_data_mask = first_band != nodata_val

    # Check if there are any valid pixels to normalize
    if np.any(valid_data_mask):
        vmin, vmax = np.percentile(first_band[valid_data_mask], (2, 98))
        clipped_band = np.clip(first_band, vmin, vmax)

        # Avoid division by zero if all valid pixels are the same value
        if vmax - vmin > 0:
            normalized_band = (clipped_band - vmin) / (vmax - vmin)
        else:
            normalized_band = clipped_band
    else:
        print("Warning: No valid data in the cropped AOI for plotting.")
        normalized_band = first_band # Plot the empty crop

    # --- 6. Plot the Raster and Overlay the *filtered* 'gdf_fully_inside' ---
    fig, ax = plt.subplots(figsize=(10, 10))

    extent = [cropped_transform[2], cropped_transform[2] + cropped_transform[0] * cropped_image.shape[2],
              cropped_transform[5] + cropped_transform[4] * cropped_image.shape[1], cropped_transform[5]]

    ax.imshow(normalized_band, cmap='gray', extent=extent)

    # --- MODIFIED PLOT ---
    # Overlay the new 'gdf_fully_inside'
    if not gdf_fully_inside.empty:
        gdf_fully_inside.plot(ax=ax, facecolor='none', edgecolor='yellow', linewidth=1.0)

    ax.set_title("Normalized Capella image with Building Footprints")
    ax.set_xlabel("Longitude")
    ax.set_ylabel("Latitude")
    plt.show()

except rasterio.errors.RasterioIOError:
    print(f"Error: Could not open the file at '{file_path}'.")
    print("Please make sure the file path is correct and the file exists.")
except ValueError as e:
    print(f"Error: {e}")

gdf_fully_inside['label'] = 0

import uuid

# Create UUIDs and set as index
uuids = [str(uuid.uuid4()) for _ in range(len(gdf_co))]
gdf_co.index = uuids
gdf_co.index.name = 'id'

# Or reset to column
gdf_co = gdf_co[['geometry']]
gdf_co.head()

# Remove existing dataset if needed:

import shutil
shutil.rmtree('marshall_test_dataset')

RASTER_PATH = f4

OUTPUT_DATASET_DIR = 'marshall_test_dataset'

# 3. Call the main function
prepare_sar_dataset(gdf_fully_inside, RASTER_PATH, OUTPUT_DATASET_DIR, target_size=(224, 224),
                          split_ratios=(0.0, 0.0, 1.0),
                          random_state=42)

import pandas as pd
from ultralytics import YOLO
import os

# 2. Define the path to your folder of images
image_folder = 'marshall_test_dataset/test/0'

# 3. Run prediction
results = model(image_folder, verbose=False) # verbose=False silences the per-image logs

# 4. Create a list to hold your data
prediction_data = []

# 5. Iterate through the results
for res in results:
    # Get the image filename
    image_id = os.path.basename(res.path)

    # Get the index of the top prediction
    top1_index = res.probs.top1

    # Get the class name for that index
    prediction_class = res.names[top1_index]

    # Append the data as a dictionary
    prediction_data.append({
        'image_id': image_id,
        'prediction_class': prediction_class
    })

# 6. Convert the list of dictionaries to a pandas DataFrame
df = pd.DataFrame(prediction_data)

# 7. Display the DataFrame
print(df.head())

df['id'] = df['image_id'].str.split('_', expand=True)[1].str.split('.', expand=True)[0]
df.head()

gdf_fully_inside.head()

merged_df = pd.merge(gdf_fully_inside, df[['id', 'prediction_class']], on='id')

merged_df.head()

# merged_df.explore(column='prediction_class')

# # Imports for streamlit
# import streamlit as st
# from streamlit_folium import st_folium
# import geopandas as gpd

# # Create a folium map
# m = merged_df.explore(column='prediction_class', legend=True)

# # Display the map in Streamlit
# st.title("Marshall Wildfire Building Damage Map")

# # st_folium renders the folium map
# st_data = st_folium(m, width=700, height=500)


# Compute map center
map_center = [merged_df.geometry.centroid.y.mean(), merged_df.geometry.centroid.x.mean()]
m = folium.Map(location=map_center, zoom_start=15)

# Add polygons with color based on prediction_class
for _, row in merged_df.iterrows():
    geojson = row['geometry'].__geo_interface__
    folium.GeoJson(
        geojson,
        tooltip=f"ID: {row['id']}<br>Prediction: {row['prediction_class']}",
        style_function=lambda feature, cls=row['prediction_class']: {
            'fillColor': 'red' if cls == 1 else 'green',
            'color': 'black',
            'weight': 1,
            'fillOpacity': 0.5
        }
    ).add_to(m)

# ------------------------------
# 5ï¸âƒ£ Display map in Streamlit
# ------------------------------
st.title("Marshall Wildfire Building Damage Map")
st_data = st_folium(m, width=800, height=600)

# ------------------------------
# 6ï¸âƒ£ Optional: Display full table
# ------------------------------
st.subheader("Full Predictions Table")
st.dataframe(merged_df[['id', 'prediction_class']])
